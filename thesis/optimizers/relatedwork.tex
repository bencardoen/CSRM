Symbolic regression fits a model, given a set of input values, to a known output. Other machine learning techniques such as Neural Networks, Support Vector Machines and Random Forests have the same functionality. Where SR distinguishes itself is in the white box nature of the model. The convergence characteristics of SR are an active field of study \citep{SRAccur}. New approaches in SR such as GPTIPS and FFX \citep{GPTIPS, FFX} focus on multiple linear regression, where a linear combination of base functions is generated. While GPTIPS still uses GP, FFX is completely deterministic and eschews GP.
In the recent work of \citep{SRlinear} SR is compared with these approaches on a series of benchmark functions. The authors concluded that while SR can have a slower convergence rate compared to conventional machine learning algorithms, the difference is not that large. Even though SR is usually associated with GP, there exists a wide variety of alternative implementations.
A non GP approach using elements from Grammatical Evolution (GE) \cite{GE}, several genetic algorithm techniques and continuous optimizers (DE, PSO) has been presented in \citep{AEG} with promising results in terms of convergence rate and accuracy. The more recently introduced ABC algorithm has also been used for SR \cite{ABCSR}. Ant Colony Optimization \citep{ACO} has been used to generate programs \citep{ACOSR}, the same functionality that allows GP to be used for SR. Several approaches to the constant optimization problem exist. The traditional solution of generating random constants \citep{GP} remains a slow approach given the size of the search space. In \citep{GPConstAlter} a structure based approach is reported to improve the random constant generation. Each constant is represented not by a single leaf node but by an evolving subtree. Apart from improved convergence, this approach also avoids hybridization of the original GP algorithm. There is a similarity with our approach, where we fold constant subtrees instead of evolving these separately. Their approach underlines our statement that evolving subtrees to generate constants can be quite effective. Our folding approach prevents vanilla GP from doing this, their approach splits constant generation from the GP algorithm but then reuses the same tree evolution techniques to evolve constants. In \citep{GPConst} the concept of numeric mutation is introduced where constants are mutated separately from the remainder of the tree using an implementation based on a temperature-biased normal distribution inspired by simulated annealing \citep{SA}. Another approach is made by applying GE to generate constants \citep{GPConstGE} where grammars are used to generate and evolve constants. Examples in the field of hybridization are DE \citep{GPDE} and PSO \citep{SRBaseline}. Advances such as semantically aware operators \citep {GPSemantics} and modularity, e.g. Koza's Automatically Defined Functions \citep{GP}, are not applied here. 