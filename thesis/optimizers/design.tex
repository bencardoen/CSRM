Our tool, Convergence of SR using Metaheuristics (CSRM), is implemented in Python3. The language offers portability, rich libraries and fast development cycles. The disadvantages are speed and memory usage compared to compiled languages (e.g. C++) or newer scripting languages (e.g Julia).
Furthermore, Python's use of a global interpreter lock makes shared memory parallelism infeasible. Distributed programming is possible using MPI.
\paragraph{Algorithm}
The algorithm accepts a matrix X = n x k of input data, and a vector Y = 1 x k of expected data. It will evolve expressions that result, when evaluated on X, in an 1 x k vector Y' that approximates Y. N is the number of features, or parameters, the expression can use. We do not know in advance whether all features are needed to build the expression.
The goal of the algorithm is to find f' such that $ d(f(X), f'(X))=\epsilon$ results in $\epsilon$ minimal. F is the process we wish to approximate with f'. Not all distance functions are equally suitable for this purpose. A simple root mean squared error (RMSE) has an issue of scale: the range of this function is [0, +$\infty$), which makes comparisons problematic, particularly if we want to combine it with other objective functions. A simple linear weighted sum requires that all terms use the same scale.
Normalization of RMSE is an option. However, there is no single recommended approach to obtain this NRMSE.
In this work we use a distance function based on the Pearson Correlation Coefficient r. Specifically, we define
\begin{equation}
\begin{aligned}
d(Y, Y') = & \\ 1 - 
 \lvert \frac{\sum_{i=0}^{n}{(y_i-E[Y])*(y'_i-E[Y'])}}{\sqrt{\sum_{j=0}^{n}{(y_j-E[Y])^2}*\sum_{k=0}^{n}{(y'_k-E[Y'])^2}}}
\lvert 
\end{aligned}
\end{equation}
R has a range of [-1, 1] indicating negative linear and linear correlation between Y and Y' respectively, and 0 indicates no correlation. The distance function d has a range [0,1] which facilitates comparison across domains and allows combining it with other objective functions. The function reflects the aim of the algorithm. We not only want to assign a good (i.e. minimal) fitness value to a model that has a minimal distance, we also want to consider linearity between Y an Y'. The use of the Pearson correlation coefficient as a fitness measure is not new, a variant of this approach is used in \citep{pearson}.
\paragraph{Genetic Programming Implementation}
We use Genetic Programming (GP) \cite{GP} to find a solution to the problem statement. The GP algorithm controls a population of expressions, represented as trees, that are initialized, evolved using operators and selected to simulate evolution.
The algorithm is subdivided into a set of phases, each phase initializes the population with a seed provided by an archive populated by previous phases or by the user. A phase is subdivided in runs, where each run selects a subset of the population, applies operators and if the application of the operator leads to fitness improvement replaces expressions in the population. At the end of a phase the best expressions are stored in an archive to seed consecutive phases. At the end of a phases the best expressions are communicated to other processes executing the same algorithm with a differently seeded population. The next phase will then seed its population using the best of all aggregated expressions.
We use a vanilla GP implementation, with 'full' initialization method \cite{GP}. Expression trees are generated with a specified minimal depth. The depth of the expressions during evolution is limited by a second parameter. GP differs from most optimization algorithms in this variable length representation. 
We use 2 operators in sequence: mutation and crossover. Mutation replaces a randomly selected subtree with a randomly generated tree. Mutation introduces new information and leads to exploration of the search space. Crossover selects 2 trees based on fitness and swaps randomly selected subtrees between them. Crossover tends to lead to exploitation of the search space. Selection for crossover is random biased by fitness value. A stochastic process decides if crossover is applied pairwise (between fitness ordered expressions in the population) or at random.
The initialization of expression trees can lead to invalid expressions for the given domain. The probability of an invalid expression increases exponentially with the depth of the tree. A typical example of an invalid tree is division by zero. While some works opt for a guarded implementation, where division is altered in semantics to return a 'safe' value if the argument is zero, we feel that this alters the semantics of the results, and is somewhat opaque to the user. Our implementation will discard invalid expressions and replace them with a valid expression. Another approach assigns a maximal fitness value to such an expression, but this can lead in corner cases to premature convergence when a few valid expressions dominate the population early on. We implement an efficient bottom up approach to construct valid trees where valid subtrees are merged. In contrast to a top down approach this detects invalid expressions early and avoids unnecessary evaluations of generated subtrees. Nevertheless, the initialization problem leads to a significant computational cost in the initialization stage of each phase and in the mutation operator.