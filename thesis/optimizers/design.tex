Our tool, Convergence of SR using Metaheuristics (CSRM), is implemented in Python3. The language offers portability, rich libraries and fast development cycles. The disadvantages are speed and memory usage compared to compiled languages (e.g. C++) or newer scripting languages (e.g Julia).
Furthermore, Python's use of a global interpreter lock makes shared memory parallelism infeasible. Distributed programming is possible using MPI.
\paragraph{Algorithm}
The algorithm accepts a matrix X = n x k of input data, and a vector Y = 1 x k of expected data. It will evolve expressions that result, when evaluated on X, in an 1 x k vector Y' that approximates Y. N is the number of features, or parameters, the expression can use. We do not know in advance whether all features are needed to build the expression.
The goal of the algorithm is to find f' such that $ d(f(X), f'(X))=\epsilon$ results in $\epsilon$ minimal. F is the process we wish to approximate with f'. Not all distance functions are equally suitable for this purpose. A simple root mean squared error (RMSE) has an issue of scale: the range of this function is [0, +$\infty$), which makes comparisons problematic, particularly if we want to combine it with other objective functions. A simple linear weighted sum requires that all terms use the same scale.
Normalization of RMSE is an option, but there is no single recommended way to do so.
In this work we use a distance function based on the Pearson Correlation Coefficient R. Specifically, we define $d(Y,Y') = 1 - R$ and R as
\begin{equation}
\begin{aligned}
 \lvert \frac{\sum_{i=0}^{n}{(y_i-E[Y])*(y'_i-E[Y'])}}{\sqrt{\sum_{j=0}^{n}{(y_j-E[Y])^2}*\sum_{k=0}^{n}{(y'_k-E[Y'])^2}}}
\lvert 
\end{aligned}
\end{equation}
R has a range of [-1, 1] indicating negative linear and linear correlation between Y and Y' respectively, and 0 indicates no correlation. The distance function d has a range [0,1] which facilitates comparison across domains and allows combining it with other objective functions. The function reflects the aim of the algorithm. We not only want to assign a good (i.e. minimal) fitness value to a model that has a minimal distance, we also want to consider linearity between Y an Y'. The use of the Pearson correlation coefficient as a fitness measure is not new, a variant of this approach has been used in \citep{pearson}.

\paragraph{Genetic Programming Implementation}
We use Genetic Programming (GP) \cite{GP} to optimize the symbolic regression problem w.r.t. the distance function above. The GP algorithm controls a population of expressions represented as trees. These are initialized, evolved and selected to simulate evolution.
The algorithm is subdivided into a set of phases. Each phase initializes the population with a seed provided by an archive populated by previous phases or by the user. A phase is subdivided in runs, where each run selects a subset of the population and applies GP operators. If this leads to fitness improvement it replaces the expressions in the population. At the end of a phase the best expressions are stored in an archive to seed subsequent phases, and are communicated to other processes executing the same algorithm with a differently seeded population. The next phase will then seed its population using the best of all available expressions.
We use a vanilla GP implementation, with 'full' initialization method \cite{GP}. Expression trees are generated with a specified minimal depth. During evolution, expressions are limited to a specified depth. We use mutation and crossover in sequence. Mutation replaces a randomly selected subtree with a randomly generated tree. This introduces new information and leads to exploration of the search space. Crossover selects two trees based on fitness and swaps randomly selected subtrees between them. This leads to exploitation of the search space. The selection for crossover is random, biased by fitness value. A stochastic process decides if crossover is applied pairwise (between fitness ordered expressions in the population) or at random.
The initialization of expression trees can lead to invalid expressions for the given domain. The probability of an invalid expression increases exponentially with the depth of the tree. A typical example of an invalid tree is division by zero. While some works opt for a guarded implementation, where division is altered to return a 'safe' value if the argument is zero, we elect to discard invalid expressions and replace them with a valid expression. We implement an efficient bottom up approach to construct valid trees where valid subtrees are merged. In contrast to a top down approach this detects invalid expressions early on and avoids unnecessary evaluations of generated subtrees. Nevertheless, the initialization constitutes a significant computational cost in the set up of each phase and in the application of the mutation operator.