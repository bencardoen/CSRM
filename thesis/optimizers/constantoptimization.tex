A constant value can appear as a linear weight for a base function, or as a leaf. The following two functions clarify the difference. $ f(x) = \sin(\log(2, x))$ and $ g(x) = 3.14 sin(x) + 4.25 cos(x)$. In f the value of 2 is a constant value but not a linear weight to a base function. In g 3.14 and 4.25 are both linear weights to sine and cosine respectively. In this work we focus on constant values that appear not as linear weights but as free constant arguments to base functions. Our tree encoding has a hidden linear weight for each node which can be optimized independently from constant arguments. When we refer to constant optimization, we are referring to the process of finding the optimal values of free constant arguments in the expression.
The size of the constant set dominates the size of the search space. We intentionally restrict this set to [0,1]. This reduces the size of c from $2^{64}$ to $2^{23}$. This restriction in range does not prevent larger constants from being evolved. The algorithm will evolve subtrees combining base functions with constants in order to find more optimal values.
An expression tree is constant if all its children are constant expression trees. As a base case, a leaf node is a constant expression if it is not a feature. This problem statement allows us to define a recursive algorithm to detect constant expressions. It should be noted that its complexity is O(n) only in the worst case, in the degenerate case where the entire tree is a constant expression. Upon detecting a non constant subtree, the algorithm returns early without a full traversal. 
Using the checking procedure in the initialization, a tree marked as a constant expression is not allowed in the initialization procedure. It is still possible to create constant expressions by applying crossover. The mutation operator will not generate constant subtrees. Our tool does not prevent constant expressions from forming in this way, the evaluation step following crossover will filter out the constant expressions using evolutionary pressure. A tree can contain subtrees that represent constant expressions. This is an immediate effect of the GP algorithm trying to evolve the optimal constant. This can lead to large subtrees that can be represented by a single node. Nodes used in such a constant subtree are not available for base functions and therefore present a waste in memory and evaluation cost. There is a counterargument to be made here: parts of a constant subtree can help evolve constants faster than a pure random selection. It is therefore possible that folding such subtrees can lead to worse fitness values. We can use a depth sensitive objective function to try to mitigate this effect, but a more direct approach is replacing the subtrees. Using the previous constant expression detection technique we can collect all constant subtrees from a tree and it replace, or fold them with the constant value they represent.
\begin{figure}
	\centering
    \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/prefold.png}
    \caption{Tree before subtree folding.}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/postfold.png}
    \caption{Tree after subtree folding.}
	\end{subfigure}
	    \caption{Constant subtree folding.}
    \label{fig:folding}
\end{figure}
We found that counterintuitively constant folding can have a negative effect on convergence in some cases. A constant subtree with depth d represents between d and $2^{d+1}-1$ constants. Both mutation and crossover can use these constants to generate more expressive trees. Constant folding is a trade-off between this expressive power and time and space complexity. More importantly, constant folding is a necessity in order to optimize the constant values with a continuous optimizer. It reduces the dimensions of the optimization problem significantly. In Figure \ref{fig:folding} we see the effect of applying the constant subtree folding. 
\subsection{Optimizers}
Using a continuous optimizer in combination with GP is a known solution \cite{GEDE, GPConst}.
Selecting an algorithm to combine with GP is a difficult question. To our knowledge there is no comparison made between optimizaton algorithms to find out which is a better fit to combine with GP. Given a tree with k constant leaves, with all constant subtrees folded, we would like to find the most optimal values for those constants resulting in a better fitness. In most optimization problems the initial solution is not known, only the problem domain and an objective function. The problem we face here does have an initial solution, namely that generated by GP. Instead of choosing random points in the search space, we therefore opt by perturbing this initial solution. In order to keep the comparison between the algorithms fair they are each configured as similar as is possible. Based on the optimal value for PSO \cite{PSO} we use a default population of 50. The iterations are set at 50. This means the cost of the optimizer will quickly dominate that of the main GP algorithm. We can apply the optimizer on each expression each iteration, only on a selection of the best expressions at each iteration, or only at the end of a phase on all, a selection, or only the best expressions. In our experiments we will show the effect of these choices on convergence.
\subsubsection{ABC}
Artificial Bee Colony \citep{ABC} is a relatively new nature inspired optimization algorithm. It is not limited to continuous optimization, and has even been used for symbolic regression itself \cite{ABCSR}. One of its key advantages over other algorithms is a lower parameter count. ABC is good at exploration (thanks to the scouting phase) though sometimes lacking in exploitation. To resolve this ABC can be combined with more exploitative algorithms \citep{ABCPSO}.
ABC is a population based algorithm, using 3 distinct phases per iteration. The algorithm maintains a set of potential solutions and a population of particles. Each particle is either employed, onlooker, or scout. An employed particle perturbs a known solution, and if an improvement in fitness is obtained replaces the old solution. If this fails a preset number of iterations, the solution is discarded and a new one scouted. Scouting in this context is generating a new potential solution. After the employed phase the onlooking particles decide, based on a fitness weighted probability, which solutions should be further optimized. In contrast to the employed particles they swap out solutions each iteration, whereas an employed particle is linked to a single solution. Finally, exhausted solutions are replaced by scouted solutions.
The algorithm initializes its solution set by perturbing it. Perturbation is done by multiplying each constant with a random number $\in [-1,1]$. Each instance records its best solution. In most optimization problems good starting positions are not known and thus a random point is selected. In our problem we already have a reasonably good solution, so we use this initial value as the mean of the normal distribution and generate values within 2 standard deviations. A configurable scaling factor is introduced, in our test problems we use 20. This value is a trade-off between exploration and an increasing probability for generating invalid solutions.  A solution is updated if the fitness value is improved. The new fitness weights for the roulette wheel selection are calculated and the global best is updated. Unlike PSO a solution is only updated if an improvement is measured. In contrast to DE, equal fitness values do not lead to an update. An equality update allows an optimization algorithm to cross zero gradient areas in the fitness landscape. The influence other solutions have on each other in ABC is not as great as in DE. In PSO the entire position, in all dimensions, is updated. In DE this depends on a parameter. This distinction can have a large impact on convergence. 
We do not know in advance if our problem instances are separable or not. Related to this discussion, PSO can be sensitive to a bias along the axes of the dimensions \cite{PSOBias} with improvements suggested in recent work \cite{PSOBiasAlg}. The following is the configuration for ABC our tool uses : 
\begin{itemize}
\item limit = 0.75 * onlookers / 2 : If a solution cannot be improved after this many iterations, it is marked exhausted and will be scouted for a new value. This limit is scaled by the number of dimensions per instance. 
\item population = 50 
\item onlookers = 25 : The number of onlookers, instances that will be assigned solutions to exploit based on fitness values. Setting this value to half that of the employed finds a balance between exploitation and evaluation cost.
\item employed = 50 
\item scouts = 25 : This is a maximum value, up to this number are used to scout after a solution is exhausted. 
\end{itemize}
This configuration is guided by the findings in \cite{ABC}.
\subsubsection{PSO}
Particle Swarm Optimization \cite{PSO} is one of the oldest population based metaheuristics. It consists of n particles that share information with each other about the global best solution.
Each particle is assigned an n dimensional position in the search space. A particle's position is updated using its velocity. This last is influenced by information from the global best and the local best. The concept of inertia is used to prevent velocity explosion \cite{PSOExplosion}.
Each particle is given a random location at start, similar to our ABC implementation we perturb the known solution with a small random value $\in$ [0,1].
To avoid premature convergence each particle is assigned a small but non-zero velocity. Without this velocity all particles are immediately attracted to the first global best. Each particle is assigned an inertiaweight which tackles the velocity explosion problem in PSO. Finally the global best is recorded. 
The algorithm updates all particles in sequence by updating its velocity using a weighted sum based on local and global best, then updating the position using that velocity.Finally it records the new global best.
In our implementation we will halt the algorithm if it cannot improve the global best after k/2 consecutive iterations, where k is the maximum number of iterations it can execute. The initialization stage adds another n evaluations, in addition to n per iteration. The total cost in fitness evaluations is therefore n(k+1), resulting in a worst case evaluation complexity of O(nk).
The following is the configuration our tool uses for PSO:
\begin{itemize}
\item $C_1, C_2$ = 2, weights for global and local best position difference. Setting both to 2 is recommended as the most generic approach \cite{PSOParameter}.
\item $w_i = \frac{1 + r}{2}$ with r random in [0,1] : In early implementations the inertia weight was kept constant \cite{PSOInertiaShi} A random inertia weight has been shown \cite{PSOInertia} to lead to faster convergence for a generic problem set.
\item $R_1, R_2$ r with r random in [0,1], random weights for the global and local best positions. 
\item population = 50 : PSO is not sensitive to populations larger than this value, providing a robust default value. \cite{SwarmIntelligence}
\end{itemize}
CSRM's optimizer does not set constraints on the domain of each constant, these are unique to each problem instance. 
Finding the domain of a constant in the expression tree requires a domain analysis of the expression tree. Finding the exact domain is infeasible, given that some of the datapoints for features are unknown (e.g. validation or test date). It is therefore possible that a particle obtains values outside the valid domain of one or more constants, resulting in an invalid expression tree. This will result in the particle temporarily no longer contributing to the search process.
\subsubsection{DE}
Differential Evolution is a vector based optimization algorithm, or rather as the name implies, it operates by computing the difference between particles. The algorithm has a population of n vectors, similar to the other algorithms it holds a linear set of values to optimize, one per dimension. Similar to our approach in initialization PSO, we perturb a known (sub)optimal solution. A vector stores its current value, and the best value. CSRM uses a DE/rand/2/bin configuration. This configuration is referenced \cite{DE} as one of the most competitive for multimodal problems with good convergence characteristics to the global optimum. Each iteration the algorithm processes all vectors. First a mutation step obtains a new vector based on the difference of 2 randomly selected (with replacement). Then we apply a binomial crossover operation. Finally we compare the new vector with the existing and replace it if it has the same or better fitness, in contrast to PSO and ABC. This equality update allows DE to traverse zero gradient areas in the search space.
This overview gives the values of each parameter used in CSRM's DE implementation.
\begin{itemize}
\item F = 0.6 : Weight determines the mutation step. The value of 0.6 is reported as a good starting value\citep{DESurveyLatest}. 
\item Cr = 0.1 : The Cr value influences the crossover step, it should be in [0,0.2] for separable functions, and [0.9, 1] for non separable functions. We cannot assume dependency between the constants, and therefore use a value of 0.1. This results in DE focussing along the axes of each dimension in its search trajectory. 
\end{itemize}
Compared to PSO DE has a low parameter count, optimal values for these parameters can be found in literature \cite{DESurveyLatest}. The population size should be t * d with t in [2, 10]. Since we do not know d in advance, and to keep the comparison fair we set the population at 50, allowing for optimal values for up to 25 dimensions (constants).