Symbolic regression (SR) fits symbolic expression to a dataset of expected values. Amongst its advantages over other techniques are the ability for a practitioner to interpret the resulting expression, determine important features by their usage in the expression, and an insight into the behavior of the resulting model (e.g. continuity, derivation, extrema). SR combines a discrete combinatoric problem (combining base functions) with a continuous optimization problem (selecting and mutating constants). One of the main algorithms used in SR is genetic programming. The convergence characteristics of SR using GP are still an open issue. GP does not lend itself well to the last aspect. This paper will study convergence of a GP-SR implementation on selected use cases known for bad convergence. We revisit the constant optimization problem by hybridizing with 3 metaheuristics: PSO, DE and ABC. We test our approach using benchmark problems known to be hard problems for SR.