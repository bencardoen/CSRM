Symbolic regression (SR) fits symbolic expression to a dataset of expected values. Amongst its advantages over other techniques are the ability for a practitioner to interpret the expression, determine important features by their usage in the expression, and gain insight into the behavior of the resulting model (e.g. continuity, derivation, extrema). SR combines a discrete combinatoric problem (combining base functions) with a continuous optimization problem (selecting and mutating constants). One of the main algorithms used in SR is genetic programming. The convergence characteristics of SR using GP are still an open issue. In this paper will study convergence of a GP-SR implementation on selected problems known for bad convergence. We revisit the constant optimization problem by hybridizing with 3 metaheuristics: PSO, DE and ABC. We test our approach using benchmark problems known to be hard problems for SR.
Our results show that while hybridization can offer orders of magnitude improvement in fitness values, care has to be taken when to apply the optimizer in order to avoid overfitting. Neither of the three algorithms was optimal for all problem instances.