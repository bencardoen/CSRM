We have introduced a distributed SR tool with a delay tolerance mechanism that mitigates load imbalance between processes. We have compared three representative topologies in terms of convergence rate and speed-up. The tree topology can be used as an approximation for the grid topology with near linear speed-up and offers a process a delay tolerance equal to the distance between dependent processes. This feature improves the tree topology to scale when the process set is larger. The distributed processes approximate K fold cross validation (KCV) in order to avoid overfitted solutions without its computational cost. Our modular architecture allows the tool to be extended with new topologies, policies, and optimization algorithms. 

In a use case we have demonstrated the use our tool can be used to build a surrogate models for a simulator in an incremental approach, i.e. concurrent to the simulator running over all input data sets. For the use case, it lead to improvements in fitness and predictive value of the final model. A feedback loop between practitioner, simulator and regression tool offers savings in time while yielding insights that can be used by domain experts to enhance the model building. The use case demonstrated that intermediate solutions are able to approximate the final model's characteristics sufficiently to be of value, thus validating our incremental approach. 

The distributed results of the use case were in line with the results of the benchmarks. The grid topology obtained the highest quality solution at the lowest speed-up. The tree topology achieved a near linear speed-up at the cost of a lower quality solution. The random topology demonstrated that the incremental approach can lead to overfitting in a distributed setting. 

Future work can take any of three directions. First our distributed architecture allows for a each process to use a different metaheuristic, creating a heterogeneous set of communicating algorithms. A cooperative set of optimizations algorithms could offer an optimal solution for all problem instances by balancing the disadvantages and advantages of each algorithm. 
Second, the tool can easily be extended with new topologies and spreading policies to further investigate their effects on convergence and accuracy. Lastly, we can approximate incremental design of experiment by making use of the collapsing property of latin hypercubes. By reducing the number of parameters and fixing the other values, we could avoid the naive linear division approach we thus far have used in seeding the tool and possibly maintain the space filling characteristics of the LHD.
