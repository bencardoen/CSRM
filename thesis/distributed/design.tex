%%!TEX root = document.tex

\subsection{Algorithm}
The algorithm requires input data in a matrix X = n x k, and a vector Y = 1 x k of expected data, with n is the number of features, or parameters. It will generate and evolve expressions to obtain an 1 x k vector Y' that approximates Y when evaluated on X. We do not know in advance if all features are equally important to predict the response, which increases the complexity.
The goal of the algorithm is to find f' such that
$
d(f(X), f'(X))=\epsilon
$
results in $\epsilon$ minimal and f is the process we wish to approximate with f'.
Not all distance functions are equally well suited for this purpose. A simple root mean squared error (RMSE) function has the issue of scale, the range of this function is [0, +$\infty$), which makes comparison problematic, especially if we want to combine it with other objective functions. A simple linear weighted sum requires that all terms use the same scale.
Normalization of RMSE is an option, however there is no single recommended approach to obtain this NRMSE.
In this work we use a distance function based on the Pearson Correlation Coefficient r. Specifically, we define
\[
d(Y, Y') = 1 - 
\lvert \frac{\sum_{i=0}^{n}{(y_i-E[Y])*(y'_i-E[Y'])}}{\sqrt{\sum_{j=0}^{n}{(y_j-E[Y])^2}*\sum_{k=0}^{n}{(y'_k-E[Y'])^2}}}
 \lvert 
 \]
The correlation coefficient ranges from -1 to 1, indicating negative linear and linear correlation between Y and Y', respectively, and 0 indicates no correlation. The distance function d has a range [0,1], which facilitates comparison across domains and allow us to make combinations with other objective functions. We not only want to assign a good (i.e. minimal) fitness value to a model that has a minimal distance, we also want to consider linearity between Y an Y'. The use of the Pearson Correlation Coefficient as a fitness measure has been used in \citep{pearson}.

\subsubsection{Genetic Programming Implementation}
In the context of symbolic regression, the GP algorithm controls a population of expressions, represented as binary expression trees. After initialization, they are evolved using mutation and cross-over operators to mimic genetic evolution.
The algorithm is based on different phases in which the population is initialized based on an tree-archive populated by the user or previous phases. One phase consists of multiple generations or runs, where the GP operations are applied on a subset of the population. If this leads to fitness improvement, the population is replaced by this new set. At the end of a phase, the best scoring expressions are stored in the archive to seed consecutive or parallel phases.

We use a vanilla GP implementation with a `full' initialization method \cite{GP}. Expressions trees are generated with a specified minimal and maximal depth, which differs from most GP optimization algorithms. 
We use 2 operators: mutation and crossover. First, mutation replaces a randomly selected subtree with a randomly generated expression tree. Next, crossover selects 2 trees based on fitness and swaps randomly selected subtrees between them. A stochastic process decides whether crossover is applied pairwise (between fitness ordered expressions in the population) or at random. The combination or new expressions and recombinations enable the exploration of the search space. 

During the initialization or recombination of expression trees, it is possible to end up with invalid expressions for the given domain. The probability of an invalid expression increases exponentially with the depth of the tree. A typical example of an invalid tree is division by zero. Some approaches alters the division semantics to return a `safe' value when the argument is zero. Our implementation discards invalid expressions and replaces them with a valid expression. We implemented a bottom up approach to detect and replace invalid trees. In contrast to a top down approach, this results in an early detection and avoids redundant evaluations of generated subtrees. However, the initialization constitutes a significant additional computational cost in the initialization stage of a phase and in the mutation operator.

\subsubsection{Software}
We implemented our distributed SR-GP algorithm, CRSE, in Python. It offers portability, rich libraries and fast development cycles. The disadvantages compared with compiled languages (e.g. C++) or newer scripting languages (e.g Julia) are speed and memory footprint.
Python's use of a global interpreter lock makes shared memory parallelism infeasible but distributed programming is possible using MPI.
The source code is provided in an open source repository \href{https://bitbucket.org/bcardoen/csrm}{repository} which also holds benchmark scripts, analysis code and plots. 
The project dependencies are minimal making the CRSE tool portable across any system with Python3, pip as an installation manager and MPI.


\subsection{Distributed Algorithm}
GP allows for both fine and coarse grained parallelism. In the first case, parallel execution of the fitness function can lead to a speedup in runtime without interfering with the the search algorithm. Unfortunately, python's global interpreter lock and the resulting cost of copying expressions for evaluation makes this approach infeasible. With coarse grained parallelism, one executes multiple instances of the algorithm in parallel, which alters the search algorithm. Each process has its own phase with an expression tree population. Processes exchange their best expressions given a predefined communication topology. The topology is a key factor for the runtime and the convergence of the search process. Message exchange can introduce serialization and deadlock if the topology contains cycles. Our tool supports any user-defined topology. 

%The communication is based on messages, which are expression trees, sent from a source to a target phase population.
After each phase, a process sends its best k expressions to each target based on the communication topology. To avoid deadlock, a process sends its expressions asynchronously, not waiting for acknowledgement of receipt. As such, the sent expressions are stored in a buffer together with a boolean. After the sending stage, the process collects all messages from its source buffers, marks the messages as ``used" and executes the next phase of the algorithm. Before sending messages, the process will verify that all previous messages have been collected. Once this blocking call is complete, it can safely reuse the buffer and start the next sending stage. This introduces a delay tolerance between processes since the phase runtime between processes can vary based on different expression lengths and evaluation cost. Without a delay tolerance, processes would synchronize on each other, nullifying any runtime gains. The delay tolerance is specified as a number of phases, which enables a process to advance multiple phases ahead of a target process in the topology. 
For hierarchical, non-cyclic topologies this can lead to a perfect scaling, where synchronization decreases as the number of processes increases.

\subsection{Approximated k-fold Cross Validation}
We divide the data over k processes, each using a random sample of 4/5 of the full input-output data. Subsequently, each process divides its data by 4/5 between training and validation. As such, the distributed process approximates a k-fold cross validation. Irrespectively of the topology, each pair of communicating processes has the same probability of overlapping data. When this probability is too low, overfitting occurs and expressions from one process are likely to be invalid for other process' training data. When the overlap is too extensive, both processes will be searching the same subspace of the search space.

\subsection{Communication Topology}
The process communication topology affects the convergence characteristics algorithm, which can be expressed as concentration and diffusion.
Concentration refers to the partitioning the search space, as discussed above. Diffusion refers to the spread of information over communicating processes to accelerates the optimization process of the entire group. However, if diffusion happens instantly, a suboptimal solution can dominate other processes, leading to premature convergence.  An edge case is a disconnected topology without diffusion of information where each process has to discover highly fit expressions independently. This might be an advantage when the risk of premature convergence due to local optima is high. An approach without communication is sometimes referred to as partitioning, as one divides the search space in k distinct partitions.
The distance between processes and connectivity will determine the impact of diffusion.

\paragraph{Grid}
The grid topology is two-dimensional square of k processes, with k the square of a natural number. Each process connects to four neighboring processes. The grid allows for delayed diffusion, because to reach all processes an optimal expression needs to traverse $\sqrt{k}$ links, and all processes are interconnected.

\paragraph{Tree}
A binary tree topology acts with a root as a source and leafs as targets with unidirectional communication. For k processes, there are k-1 communication links, reducing the messaging and synchronization overhead significantly compared to the grid topology. Diffusion is restricted, because the information flow is unidirectional. On the other hand, optimal expressions are spread over the outgoing links (which is a spreading distribution policy) a partitioning effect occurs counteracting premature convergence. As there are no cycles, synchronization overhead is minimal.

\paragraph{Random}
In a random topology, the convergence of the distributed algorithm is hard to predict. Cycles and cliques are likely, thus diffusion is not guaranteed and runtime performance depends on the actual instance. As advantage, patterns that might interfere with deterministic topologies are avoided.
