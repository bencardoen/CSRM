Our tool is implemented in Python. It offers portability, rich libraries and fast development cycles. The disadvantages compared with compiled languages (e.g. C++) or newer scripting languages (e.g Julia) are speed and memory footprint.
Furthermore, Python's use of a global interpreter lock makes shared memory parallelism infeasible. Distributed programming is possible using MPI.
\subsection{Algorithm}
The algorithm accepts a matrix X = n x k of input data, and a vector Y = 1 x k of expected data. It will evolve expressions that result, when evaluated on X, in an 1 x k vector Y' that approximates Y. N is the number of features, or parameters, the expression can use. We do not know in advance if all features are needed, which makes the problem statement even harder.
The goal of the algorithm is to find f' such that
$
d(f(X), f'(X))=\epsilon
$
results in $\epsilon$ minimal. F is the process we wish to approximate with f'.
Not all distance functions are equally well suited for this purpose. A simple root mean squared error (RMSE) function has the issue of scale, the range of this function is [0, +$\infty$), which makes comparison problematic, especially if we want to combine it with other objective functions. A simple linear weighted sum requires that all terms use the same scale.
Normalization of RMSE is an option, however there is no single recommended approach to obtain this NRMSE.
In this work we use a distance function based on the Pearson Correlation Coefficient r. Specifically, we define
\[
d(Y, Y') = 1 - 
\lvert \frac{\sum_{i=0}^{n}{(y_i-E[Y])*(y'_i-E[Y'])}}{\sqrt{\sum_{j=0}^{n}{(y_j-E[Y])^2}*\sum_{k=0}^{n}{(y'_k-E[Y'])^2}}}
 \lvert 
 \]
R has a range of [-1, 1] indicating negative linear and linear correlation between Y and Y' respectively, and 0 indicates no correlation. The distance function d has a range [0,1] which facilitates comparison across domains and allows combining it with other objective functions. The function reflects the aim of the algorithm. We not only want to assign a good (i.e. minimal) fitness value to a model that has a minimal distance, we also want to consider linearity between Y an Y'. The use of the Pearson correlation coefficient as a fitness measure is not new, a variant of this approach is used in \citep{pearson}.
\subsubsection{Genetic Programming Implementation}
In the context of symbolic regression, the GP algorithm controls a population of expressions, represented as trees. These are initialized, evolved using operators and selected to simulate evolution.
The algorithm is subdivided into a set of phases. Each phase initializes the population with a seed drawn from an archive populated by previous phases or by the user. A phase is subdivided in runs, where each run selects a subset of the population and applies the GP operators. If this leads to fitness improvement, it replaces the expressions in the population. At the end of a phase the best expressions are stored in an archive to seed consecutive phases. At the end of a phases the best expressions are communicated to other processes executing the same algorithm with a differently seeded population. The next phase will then seed its population using the best of all available expressions.

We use a vanilla GP implementation, with 'full' initialization method \cite{GP}. Expressions trees are generated with a specified minimal depth. The depth of the expressions during evolution is limited by a second maximal parameter. GP differs from most optimization algorithms in this variable length representation. 
We use 2 operators in sequence: mutation and crossover. Mutation replaces a randomly selected subtree with a randomly generated tree. The introduction of new information leads to exploration of the search space. Crossover selects 2 trees based on fitness and swaps randomly selected subtrees between them. This exploits the search space. Selection for crossover is random, biased by fitness value. A stochastic process decides whether crossover is applied pairwise (between fitness ordered expressions in the population) or at random.

The initialization of expression trees can lead to invalid expressions for the given domain. The probability of an invalid expression increases exponentially with the depth of the tree. A typical example of an invalid tree is division by zero. While some works opt to alter the division semantics to return a 'safe' value when the argument is zero, our implementation discards invalid expressions and replaces them with a valid expression. We implement an efficient bottom up approach to construct valid trees where valid subtrees are merged. In contrast to a top down approach this detects invalid expressions early on and avoids unnecessary evaluations of generated subtrees. Nevertheless, the initialization constitutes a significant computational cost in the initialization stage of each phase and in the mutation operator.
\subsection{Distributed algorithm}
GP allows for both fine grained and coarse grained parallelism. Parallel execution of the fitness function can lead to a speedup in runtime, but will not affect the search algorithm. Python's global interpreter lock and the resulting cost of copying expressions for evaluation makes this approach infeasible. With coarse grained parallelism one executes k instances of the algorithm in parallel. Each process has its own population of expression trees. Processes exchange their best expressions given a predefined process communication topology. This does affect the search algorithm. The topology is a key factor in determining the runtime and the convergence of the computation. Message exchange can introduce serialization and deadlock if the topology contains cycles. Our tool supports any user provided topology and must be able to deal with both issues. 

Messages are expressions sent from a source process's to a target process's population.
After each phase a process looks up its targets using the topology. It sends its best k expressions either by copying all expressions to each target or by spreading them over the target set. Upon completion of the sending stage, the process looks up its set of sources and waits until all source processes have sent their best expressions. To avoid deadlock a process sends its expressions asynchronously, not waiting for acknowledgement of receipt. The sent expressions are stored in a buffer for this purpose, together with an associated callable waiting object. After the sending stage the process synchronously collects messages from its sources, and executes the next phase of the algorithm. Before the next sending stage, the process will then check each callable to verify that all messages from the previous sending phase have been collected. Once this blocking call is complete, it can safely reuse the buffer and start the next sending phase. This also introduces a delay tolerance between processes. The phase runtime between processes will never be exactly identical, especially not given that the expressions have a variable length representation and differing evaluation cost. Without a delay tolerance processes would synchronize on each other, nullifying any runtime gains. With the delay tolerance a process may advance k phases ahead of a target process k steps distant in the topology. 
For hierarchical, non-cyclic topologies this can lead to a perfect scaling, where synchronization decreases as the number of processes increases.
\subsection{Approximated k-fold cross validation}
We divide the data over k processes, each process using a random sample of 4/5 of the whole data. Each process effects a 4/5 split of its data between training and validation. The aggregate distributed process then approximates k-fold cross validation. Whatever the topology, each pair of communicating processes will have the same probability of overlapping data. When this probability is too low, overfitting occurs and highly fit expressions from one process will likely be invalid for another process' training data. When the overlap is too extensive, both processes will be searching the same subspace of the search space.
\subsection{Communication Topology}
The process communication topology affects the convergence characteristics algorithm. In a disconnected topology, there is no diffusion of information. Each process must discover highly fit expressions independently. An edge case where this is an advantage, is when the risk of premature convergence due to local optima is high. We can try to avoid those optima by executing the algorithm in k instances, without communication. Such an approach is sometimes referred to as partitioning, as one divides the search space in k distinct partitions.

However, in general the goal is for the processes to share information in order to accelerate the search process. With a topology we introduce two concepts : concentration and diffusion. Concentration refers to processes that share little or no information and focus on their own subset of the search space. As with an overemphasis on search space exploitation, concentration can lead to overfitting and premature convergence. It is warranted when the process is nearing the global optimum. Diffusion, in this work, refers to the spread of information over communicating processes. It accelerates the optimization process of the entire group. It is not without risk, however. If diffusion happens instantly, a suboptimal solution can dominate other processes, leading to premature convergence. The distance between processes and connectivity will determine the impact of diffusion.
\paragraph{Grid}
The grid topology is two-dimensional square of k processes with k the square of a natural number. Each process connects to four neighboring processes. The grid allows for delayed diffusion, because to reach all processes an optimal expression needs to traverse $\sqrt{k}$ links, and all processes are interconnected.
\paragraph{Tree}
A binary tree with with root as a source and leafs as sinks with unidirectional communication, is an efficient hierarchical topology. For k processes there are k-1 communication links, reducing the messaging and synchronization overhead significantly compared to the grid topology. Diffusion can be hampered, because the information flow is unidirectional. On the other hand, with a spreading distribution policy (optimal expressions are spread over the outgoing links) a partitioning effect occurs that counteracts premature convergence. As there are no cycles, synchronization overhead is low.
\paragraph{Random}
In a random topology the convergence of the aggregated process is hard to predict. Cycles and cliques are likely, thus diffusion is not guaranteed and runtime performance differs based on the actual instance. The advantage of the random topology is that certain patterns that might occur deterministic topologies are avoided.