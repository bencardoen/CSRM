In this section we will cover the parallelization of symbolic regression, and explain our design choices.
\subsection{Approaches}
We distinguish between fine grained parallelism and course grained parallelism. 
\paragraph{Fine grained parallelism}
In fine grained we parallelize a single step in the algorithm, where we execute a number of tasks in parallel that are independent of each other and have a relative short completion time. The evaluation of the fitness function is a prime example of such a workload. The fitness evaluation itself can be quite complex, but in comparison with the entire runtime of the algorithm the computational cost of a single calculation is small. In order to efficiently parallelize such a taskset we have employ a mechanism that has minimum overhead. Shared memory parallelism using threads is ideal for this use case. Optimizations suchs as dynamically allocated threadpools will reduce the overhead even further, and increase the speedup. Overhead in this case is split over the implementation overhead of starting or assigning a thread and administering it, and the copying of the problem data. Shared memory implies a near zero cost in copying, only a reference is shared. A lock on the shared data is not needed, since the instance the thread operates on is not used by any other part of the program while we execute the fitness function. 
\paragraph{Course grained parallelism}
In course grained parallelism we run a series of tasks in parallel that have a long runtime or complex workload. In our setting an example of course grained parallelism is running the entire algorithm in parallel, with several instances tackling a distinct subset of the problem as separate processes. With a longer runtime of the task, the overhead of parallelizing the problem can be significantly greater without impacting the speedup obtained. Typically we only start and stop such a task once, in contrast with fine grained task which are constantly started and then stopped again. We can use processes or threads for this form of parallelism. Threads have the benefit of being lighter in comparison to processes in terms of overhead. If we allow communication between the tasks threads can elide copying the data, at the cost of introducing locks. Processes typically have to copy data in order to communicate. We can compare both approaches with an interrupt based approach versus message passing. Whichever we choose, communication between tasks requires synchronization of some form. This introduces not only memory and time overhead, but also significantly complicates the implementation. Invariants that hold in sequential implementations are no longer guaranteed in parallel, and a careful implementation is required. 
% Add vizualization of fine versus course.
\paragraph{Our approach}
\subparagraph{Evaluating fine grained parallelism}
Python has poor support for shared memory parallelism. While threading is available, it will not offer a speedup except for IO bound tasks due to the presence of global lock in the interpreter (GIL). It is possible to use compiled extensions in C to work around this issue, at a high cost in code complexity. A prototype implementation that parallelized the fitness function calculation proved that both threading and processes are unable to offer any speedup, often even running slower than sequential. A large part of this cost is due the overhead of copying Python objects, which are reference counted and thus require a graph traversal in order to create full copies. In order to evaluate a population of n expression tree of depth between 5 and 10 in parallel, we have to first copy the n expressions to each of the n processes, then evaluate the tree, and then copy the result back. The copying operation is far more expensive than the evaluation function, and both scale exponentially with the depth of the tree. A prototype using threads elided the copy, but the GIL ensured that performance was lower than the sequential approach.
This ruled out fine grained parallelism in CSRM.
\subparagraph{Evaluating course grained parallelism}
We create k processes, and give each an instance of the algorithm to execute in parallel. Communication is done via message passing. We use the MPI framework, which offers Python bindings, to allow processes to communicate and synchronize. Copying is still costly, but now a speedup is possible. Threading did not work in this approach due to the GIL.
CSRM uses a distributed set of communicating processes in order to solve the SR problem.

\subsection{Distributed SR}
We will now discuss the benefits of the distributed (course grained) application to our problem.
A parallel metaheuristic has the obvious advantage of speedup in comparison with a sequential implementation. By dividing the problem over k processes we can, in ideal circumstances, obtain a speedup linear in k. With speedup we then refer to the time, or number of evaluations, needed to reach a certain threshold of fitness. The advantage of parallelization do not stop with this speedup. We can view the parallel processes as a swarm in itself, where each instance communicates with the others using a predefined pattern. Instead of k standalone processes we now have a set of k cooperating  processes. Each process can now use the information of others to improve its own search. Using this approach a superlinear speedup is possible. There are, however, downsides. Communication implies overhead, not only in memory needed but also in synchronization. Without communication a linear speedup is only dependent on the implementation. 
With communication we introduce time constraints which can bottleneck a subset of the processes, or in the worst case serialize the entire group. We would like to exchange the most valuable information with the least amount of overhead. This balance is problem specific, the cost of copying depends on what exactly is being copied when an to whom it is sent. Even if we restrict us to 1-1 link between two processes, and only exchange the fittest expressions between the two processes, we do not know in advance how large the expression we copy will be, as the depth and sparseness of the tree representing the expression will vary.
While each process is given an equal sized subproblem, there are no guarantees that the actual workload of the different processes will be equal. We know that the evaluation of the fitness function has variable computational load. The stochastic nature of the metaheuristics used in the SR implementation compound this issue. By virtue of the problem statement we do not know the optimal 
solution to our problem, and with different starting points convergence between the different processes is likely to differ. We will address each of these issues.

\subsection{Topology}
A topology in this context is the set of communication links between the processes. The topology determines the convergence behavior of the entire group. In a disconnected topology, there is no diffusion of information. If a process discovers a highly fit expression, that knowledge has to be rediscovered by the other processes. The only edge case where this is an advantage is if we see the group of processes as a multiple restart version of the sequential algorithm. If the risk for premature convergence due to local optima is high, we can try to avoid those optima by executing the algorithm in k instances, without communication. 
The implementation should offer the process an efficient way to lookup both processes from which it will receive information (sources) and processes it has to send to (targets). Source lookup is needed in order to solve the synchronization problem. Any topology that contains a cycle between processes can introduce a potential serialization effect at runtime.
\paragraph{Diffusion and concentration}
In general would like the processes to share information in order to accelerate the search process. With a topology we introduce two concepts : concentration and diffusion. Concentration refers to processes that share little or no information, and focus on their own subset of the search space. Like exploitation concentration is sensitive to overfitting and premature convergence. It is warranted when the process is nearing the global optimum. Diffusion, in this work, is the process of sharing optimal solutions with other processes. Diffusion accelerates the optimization process of the entire group. It is not without risk, however. If diffusion is instant, a single suboptimal solution can dominate the other processes, leading to premature convergence. 
The topology will determine the synchronization between processes, and the balance between diffusion and concentration.
\paragraph{Synchronization}
Synchronization between the processes will play an important role. While it won't directly influence convergence, it will constrain the runtime performance of the entire group. If we denote $S_i$ and $T_i$ as the set of processes that are sources and targets respectively for process i, we would like $S_i$ minimal. The message processing code will have to wait for the slowest processes in $S_i$ before continuing. Without asynchronous communication i would have to wait even longer, with the slowest process blocking the receipt of messages from the other processes. Synchronization implies that i cannot send until its receiving stage has completed. If $S_i \cap T_i \neq \emptyset$ we have a cyclic dependency which can introduce deadlock in the implementation. By extension, if there is a cycle in the topology between any i, j processes this deadlock is a real risk. Dealing with this in the communication code is non trivial. We will show in section \ref{subasync} how CSRM is able to deal efficiently with cycles in the topologies that it implements. Even with deadlock solved or prevented, cycles will introduce a tendency for the the processes to serialize on the slowest process in the node. The time spent waiting in the communication stage is lost to the convergence process. We will show how this is mitigated in our implementation.
\subsubsection{Grid}
This topology arranges a set of k processes in a square 2D grid. Each process is connected with the neighbouring processes, respectively up, down, left and right. Some variations include diagonal links, or create 3 or higher dimensional meshes. The general idea behind the grid is unchanged in those configurations. 
A grid connects all processes, allowing for diffusion of the best solution to each individual process. The key observation here is that diffusion is gradual. While a process has an immediate neighbourhood, reaching all processes takes a variable amount of communication links. Diffusion of a dominating solution will take time, and if the solution is suboptimal this time allows the other processes to evolve their own optimal solutions thereby preventing premature convergence. This risk is only mitigated by gradual diffusion, not eliminated. Elimination is only possible in the extreme case by an absence of communication or in a more advanced configuration the usage of cliques. 
Nodes on the borders of the grid communicate with their counterparts at the mirrored side of the grid. This ensures that the communication process is symmetric. CSRM supports both square grids, where k is square of natural number, or incomplete squares. 
If $\sqrt{k} \neq n $ for some $n \in \mathbb{N}$ we create a grid that would fit j processes where $ \min(j) > k \forall j \in \mathbb{N} \land \sqrt{j} = n$. This grid is filled row by row with k processes. This configuration should be avoided, as the communication pattern will not always be symmetrical. It is implemented to allow for a fair comparison with other topologies where the processcount is not a square. 
% TODO Figure.
\paragraph{Cost}
The communication cost for k processes in a single iteration, with m messages sent per exchange, is in our configuration 4km.
The synchronization constraints are high, all processes are interdependent (directly or at most $\sqrt{k}$ links removed).
Lookup of targets is statically determined, at configuration each parallel process is given a simple integer indexed mapping, and the symmetry of the mapping simplifies the lookup code.
\subsubsection{Wheel}
A wheel, or circle topology connects all processes with a single link shared between each source and target. Diffusion is slower than with a grid, with k processes it takes k-1 messages to reach all processes. Some variations introduce a 'hub', with spokes reaching out the circle itself, completing the wheel analogy. CSRM implements this topology as a simple circle. A spoked wheel topology, if the hub has bidirectional communication with all processes, has a maximum distance of 2 between all processes offering fast diffusion. Without a hub but bidirectional links the maximum distance is k/2.
\paragraph{Cost}
For k processes with m messages sent per exchange, the communication cost is km. This is a static configuration, with symmetric lookup. A circle forms a cycle, resulting in high synchronization constraints. The variant with hub and spokes introduces even higher synchronization constraints. At a doubling of message cost the doubly linked circle has an significantly higher synchronization cost than the singly linked variant. 
% Figure
\subsubsection{Random}
In a random topology a process picks a random target for its messages. We can either configure it to do so statically, such that the target remains invariant at runtime, or select a new target each iteration. The number of targets is variable as well. CSRM implements all variants, allowing for both dynamic and static random topologies with a variable amount of targets per sending process. The idea behind a random topology is avoiding patterns that are present in the structured topologies. If such a pattern leads to premature convergence, poor synchronization or fails to gain from the exchange of information between the processes there exists a possibility that a random approach can work. The downside is that we do not know what the maximum distance is between two processes, or even if they are connected. There can cliques in the topology, or cycles. Avoiding these, or detecting them requires more complex code than a simple random assignment of targets. By increasing the number of targets we decrease the possibility of cliques, and will minimize eventually the distance between two processes, at the cost of increased overhead.
CSRM does not enforce constraints such as clique or cycle forming in its random topologies.
\paragraph{Cost}
The cost for a singly linked random static topology of k processes with m processed per exchange is clearly km. Synchronization constraints are unknown, and would have to be resolved by the processes. Cyclic dependencies between the processes are likely.
The lookup code for a static configuration is simple, symmetric and fast. For a dynamic configuration the lookup is simple, but only symmetric at single points in time. This observation is important because the virtual time of a process will diverge from that of the other processes. By allowing a dynamic configuration we introduce a new type of synchronization constraint. In CSRM processes have a copy of the global topology, but this is no longer immutable. In order to resolve sources the process has to know the virtual time of the other processes. The topology remains deterministic, the seed used is identical between each process. Synchronization is highly complex in a random dynamic topology as cycles and cliques vary over time.
% Figure
\subsubsection{Tree}
We introduce a binary tree topology. The links are unidirectional, with a single root and a given depth. The processcount should ideally be $k = 2^{d+1} -1$ for some $d > 0 \in \mathbb{N}$ to create full binary tree. This is not a hard constraint, with values not satisfying we construct a tree of depth $ d =\lfloor\log_2{k}$ and fill the last level left to right until all processes are assigned a position. Communication is unidirectional, from the root to the leaves. This topology is the only one free from cliques or cycles. The leaves act as sinks, while the root is a source. Lookup is fast, static and symmetric. Each process, except the leaves has at most two targets, and one source (except the root). The tree topology offers a structured balance between diffusion and concentration. The distance between two processes range from 1 to $\log_2{k}$. In the case of leaves the distance is infinite. Note that depending on the communication strategy, each subtree behaves as a sink, and will not diffuse information.
A variant on this topology is a singly linked list, where the maximum distance is k. The problem with this topology is that its diffusion scales linearly with k.
\paragraph{Cost}
For k processes and m messages per exchange, we have km messages exchanged. Synchronization overhead is minimal, there are no cycles and a process is influenced at most by $log_2{k}$ other processes and influences at most k-1. With the exception of a disconnected topology the tree topology allows for a high speedup compared with grid, random and circle. 
%Figure
\subsubsection{Disconnected}
The disconnected topology has zero diffusion and an infinite distance between processes. The only applications of this topology are in cases where premature convergence due to diffusion is known in advance and can for some reason not be mitigated, and as a comparison with the other topologies in order to measure memory and synchronization cost. 
\paragraph{Cost}
Cost is near zero, no messages are sent nor is any synchronization needed. In practice the collecting of all results will still have to be done by either an elected process or a process statically assigned as collector. This holds true for all topologies.
% Figure
\subsection{Asynchronous communication}\label{subasync}
We have to tackle deadlock and synchronization delay in our parallel implementation. We will first describe the interaction between the processes and using this context demonstrate our solution.
\paragraph{Control flow}
% TODO control flow parallel image
A parallel process in our implementation executes a single phase of the algorithm, then collects at most m of its fittest expressions from its archive.
The set of target processes is resolved using the topology, and then the m messages are send to the targets. 
After the sending stage the process looks up its sources using the topology, and waits for messages from those sources. 
After receiving the messages are decoded into solutions which are passed to the algorithm for its next phase.
\paragraph{Waiting for messages}
If the process does not wait for messages from the other processes the convergence behavior of the entire group becomes extremely hard to predict. In most platforms there are no hard guarantees about inter process scheduling. We can end up in extreme cases with a single process only sending and never consuming messages. This would break the intention of any structured topology. While this is an extreme example, even in average causes an extra level of non determinism is introduced without there being an explicit need for it. In addition, not waiting for messages requires that messages are collected by the parallel framework, and this can lead to internal buffers varying strongly in size. The sending code would have to be asynchronous as well. We will show another approach where the deterministic execution of the parallel metaheuristic is retained. This does not imply that the process itself is no longer stochastic, only that we are guaranteed that our designed communication pattern is strictly adhered to.
\paragraph{Deadlock}
From our previous discussion we know that deadlock is a risk if there are cycles in the topology. Since each process has full knowledge of the topology (except in a random dynamic topology) it is possible to write an implementation that solves deadlock by ordering the sending an receiving of the messages. Suppose A waits for B, and B for A in the most simple example. A simple solution of A sending to B, B receiving from A, then B sending to A and A receiving from B would break the deadlock. This simple interleaving solution is not unique, but the priority is not important here. As long as the deadlock is broken the processes can continue. There are 2 serious issues with this approach. First, this approach serializes concurrent processes. This is detrimental to any speedup we were hoping to achieve. Second, the implementation becomes more complex. The order of calls is no longer statically defined, we need an algorithm that acts as a central coordinator between any group of processes in a cycle, computes a solution, and executes that solution in order. We cannot directly invoke operations on other processes, so the only solution is to execute the coordinating algorithm in parallel on all processes if they detect that they are a member of a cycle. Each process will then know when it can send an receive based on the computed order. While this solves deadlock, we still end up with a serialized execution and the communication code becomes far more complex than a simple sequential sending and receiving call.
\subparagraph{Asynchronous solution}
CSRM instead opts for an asynchronous sending of messages in order to resolve deadlock and mitigate the serialization. Cycle detection is no longer needed. 
A process executes a phase, then looks up its targets for messages. Instead of sending the messages with a blocking call to the framework, the sending process now allocates a buffer and send each set of messages to its target with an asynchronous call. It stores future object that can later be checked to verify if the receiving process has executed its mirror receive call. 
The order of sending is no longer relevant, the sending calls return immediately for all targets. 
Next the process calls receive for each of its sources. This is a blocking call, but this does not introduce a risk for deadlock.
In the worst case our process waits until the source has stopped its own sending calls, but due to its asynchronous nature this process is quite fast. The order of sources is again no longer a risk for deadlock. After receiving all messages the process continues its execution as before.
% Explain buffers and futures.
\paragraph{Synchronization delay}
We know from our discussion that the execution time of a single phase will vary per process, and even between phases for a single process. 
% What is it
% Given our discussion of the need for synchronization etc etc etc
% Why do we need it
% What's the possible gain
% How do we implement it
\paragraph{Future improvements}
% both sending and receiving async.
\subsection{Overhead}
% The key analysis.
\subsection{Communication strategies}
If the process has j targets, and the algorithm is configured to distribute the m best solutions, we can use several approaches to send those to their targets. % Spreading
% What do we exchange and to who, when  and how much ?
% Spreading factor
% Why does this matter ?
\subsection{K fold sampling}
% What is it ?
% How do we use it ?
% Why use it here ?