\subsection{Constant Optimization} \label{subconstantoptimization}
Selecting base functions is a combinatoric, discrete problem. Selecting the right constants to use is a continuous problem, that GP tries to solve with a combinatoric approach. There are several aspects to this issue, we will go over each individually and show our approach.
\subsubsection{Initialization revisited}
During initialization it is possible that a tree represents a constant expression. Such an expression is only a valid approximation if no feature has an influence on Y, which is an extremely unlikely edge case. A constant expression is of no use for the algorithm, but without measures to prevent or remove these they will still be formed. Ideally such an expression will have a high (worse) fitness value than non constant expression, and will be eventually filtered out. Since detecting a constant expression is feasible in worst case O(n) with n the number of nodes in the tree, a more efficient approach is preventing constant expressions from being generated. 
\paragraph{Constant expression detection}
An expression is a constant expression if all its children are constant expressions. As a base case, a leaf node is a constant expression if it is not a feature. This problem statement allows us to define a recursive algorithm to detect constant expressions. It should be noted that its complexity is O(n) only in the worst case, when the tree is a constant expression. Upon detecting a non constant subtree, the algorithm returns early without a full traversal. 
\paragraph{Preventing constant expressions}
Using the checking procedure in the initialization, a tree marked as a constant expression is not allowed in the initialization procedure.
It is still possible to create constant expressions by applying mutation and crossover. If the left subtree of a node is a constant expression, and the right is not constant, but this right is replaced by either mutation or crossover with a constant expression then the node becomes a constant expression. The mutation operator will not generate constant subtrees, so this leaves only crossover. Our tool does not prevent constant expressions from forming in this way, the evaluation following crossover will remove the constant expressions immediately.
\subsubsection{Folding}
% Future work : fold the multipliers

\paragraph{Constant subtree problem}
A tree can contain subtrees that represent constant expressions. This is an immediate effect of the GP algorithm trying to evolve the correct constant. This can lead to large subtrees that can be represented by a single node. Nodes used in such a constant subtree are not available for base functions. They waste memory and evaluation cost, without their presence the tree could become a fitter instance. We will evaluate these effects in the experiments section.

\paragraph{Constant subtree folding}
It is possible to use a depth sensitive objective function to try to mitigate this effect, but a more direct approach is replacing the subtrees.
Using the previous constant expression detection technique we can collect all constant subtrees from a tree. We evaluate each subtree, and replace it with the constant value it represents. This leads to a saving nodes, possibly in depth. These savings can have an effect on the convergence as well. Mutation and crossover will no longer operate on constant subtrees, and the iterations and place in the tree that becomes available can be used to improve the tree.
Constant folding requires an O(n) detection check, since the entire tree needs to be traversed. The folding operation itself is at worst O(n), if the entire tree is constant.
% Figure
\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/prefold.png}
    \caption{Tree before subtree folding.}
    \label{fig:prefold}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{figures/postfold.png}
    \caption{Tree after subtree folding.}
    \label{fig:postfold}
\end{figure}

\paragraph{Edge cases}
In Figure \ref{fig:postfold} we the effect of applying the constant subtree folding. 
There are subtle edge cases that can't be detected using the above method. Consider the tree representing the expression
\[
f(x) = \max(x, 4 + \sin(0.3)) 
\]
% Link to image
The subexpression 4 + sin(0.3) is detected and folded into the constant 4.26. 
We should not stop there, since f can still be a constant expression if $\forall i :  x_i < 4.26$. To verify this we need to evaluate f on all i datapoints. In contrast, the constant subtree detection code needs only 1 evaluation. In the figure we see a similar case in the right subtree, for the first value of $x_0$ the right subtree is indeed constant. 
Even if we should evaluate f for all i, this will not guarantee us that f is indeed a constant expression. All this check then proves is that for all i in the training data, f is constant. The testing data holds new values, for which f may or may not be constant. We conclude that this edge case cannot be prevented from occurring.
Another similar case occurs in expressions of the form 
\[
f(x, y) = \tan(y) * \sqrt{x/x}
\]
In this reduced example the node $\sqrt{x/x}$ is simply $\sqrt{1}$. Outside of this example, detecting such cases is non trivial. There is clear benefit to do so, despite their low occurrence: if the domain of x includes 0 this tree is never generated because it leads to division by zero in a single datapoint. Discarding this expression is however not needed, since $\sqrt{1}$ is a valid subexpression. One way to solve this is to use mathematical software to simplify the expressions, and then convert them back to tree representations. Applying this step should be based on the cost versus benefit and the frequency of such occurrences. 
% link future work

\paragraph{Conclusion}
Constant detection and folding mitigate some of the side effects of the constant optimization problem, they do not lead to a more efficient finding of the 'right' constant values. For this we need a continuous optimizer, algorithm designed specifically to optimize real valued problems.

\subsection{Optimizers}

\paragraph{Hybridizing GP}
Using a real valued optimizer in combination with GP is a known solution \cite{GEDE, GPConst}.
Which algorithm to combine is a difficult question. To our knowledge there is no comparison made between optimizaton algorithms to find out which is a better fit to combine with GP. 

\paragraph{Problem statement}
Given a tree with k constant leaves (since all constant subtrees are folded), we would like to find the most optimal values for those constants resulting in a lower (better) fitness. 
It is vital to perform the constant folding step before optimization takes place. Suppose a given tree has on average k constants, which after folding become j with j $<=$ k. Without folding the optimizer has to solve a k dimensional optimization problem, whereas after folding the problem is only j dimensional. The underlying problem only has j dimensions, so this step is a necessity.

\paragraph{Initialization}
In most optimization problems the initial solution is not known, only the problem domain and an objective function. The problem we face here does have an initial solution, namely that generated by GP. Instead of choosing random points in the search space, we therefore opt by perturbing this initial solution. An immediate problem here is that the optimizer may simply converge on the initial solution. This risk can be high, given that GP already has evolved it as a suboptimal solution. 
The problem faced by the population initialization step \ref{subsubinvalidexpressions} reappears here. We could pick random values in the search space, but these are likely to generate invalid trees. We need a balance between exploration and exploitation here. 

\paragraph{Domain}
Each tree instance has a completely different domain for each constant. We cannot therefore guide the optimizer with domain specific knowledge. This also reflects in the choice of parameters for the optimizer, which will have to be suboptimal for specific problem instances as we want them to work on the largest set of problems.

\paragraph{Comparison and configuration}
In order to keep the comparison between the algorithms fair they are each configured as similar as is possible. Since they are all population based, and given a maximum number of iterations, all three share these same parameter values. Each application of the algorithm has a significant cost in comparison with the main GP algorithm. In a single iteration with population p, the GP algorithm is likely to perform 2n evaluations (mutation and crossover). If we give the optimizer a population of m and q iterations, it will execute at most in the order of  m x q evaluations. Based on \cite{PSO} we use a default population of 50, and 50 iterations. This means the cost of the optimizer will quickly dominate that of the main GP algorithm, depending on when and on what we apply it.

\subsubsection{ABC}
% What is it
% Why DE
% How Configure it
Artificial Bee Colony \citep{ABC} is a relative new nature inspired optimization algorithm. It is not limited to continuous optimization, and has even been used for symbolic regression itself \cite{ABCSR}. One of its key advantages over other algorithms is a lower parameter count. Optimal values for an optimization algorithm have a large impact on its convergence behavior, so much so that other optimizers can be required to find the parameters of an optimizer. With a small parameter set finding optimal values becomes easier. Finding optimal values for these parameters is quite often related to the problem domain, and as we have seen each instance here will have a new domain. ABC is good at exploration (thanks to the scouting phase) though sometimes lacking in exploitation. To resolve this ABC can be combined with more exploitative algorithms \citep{ABCPSO}.

\paragraph{Algorithm}
ABC is a population based algorithm, using 3 distinct phases per iteration. We will refrain from using the nature analogy in describing the algorithm as it is quite often inappropriate. The algorithm maintains a set of potential solutions and a population of particles. Each particle is either employed, onlooker, or scout. An employed particle perturbs a known solution, and if an improvement in fitness is obtained replaces the old solution. If this fails a preset number of iterations, the solution is discarded and a new one scouted. Scouting in this context is generating a new potential solution. After the employed phase the onlooking particles decide, based on a fitness weighted probability, which solutions shoud be further optimized. In contrast to the employed particles they swap out solutions each iterations, whereas an employed particle is linked to a single solution. Finally exhausted solutions are replaced by scouted solutions.

\paragraph{Configuration}
We use a solution set of 50, a population of 25 employed particles, 25 onlookers and a single scout. These parameters are suggested in \citep{ABC}. A higher scout value is not warranted, since we already have a good starting point. More exploration would also result in the initialization problem dominating the runtime cost of the optimizer. 

% TODO
\subsubsection{PSO}
Particle Swarm Optimization \cite{PSO} is one of the oldest population based metaheuristics. It consists of n particles that share information with each other about the global best solution.

\paragraph{Algorithm}
\subparagraph{Initialization}
Each particle is assigned a n dimensional position in the search space, in our application a perturbed instance of the constant values we wish to optimize. A particle's position is updated using its velocity. This last is influenced by information from the global best and the local best. The concept of inertia is used to prevent velocity explosion \cite{PSOExplosion}.
Each particle is given a random location at start. In our application we already have an (sub)optimal solution, the constant values in the tree instances have been evolved by the GP algorithm. Rather than pick random values, we perturb the existing solution. This is a difficult trade-off. If the perturbation is not large enough the optimizer will simply converge on the known solution. If we perturb too greatly the risk for invalid solutions increases, rendering a large selection of the population invalid. We can initialize the population with n perturbed solutions or with n-1 with the initial value remaining intact. The n-1 solution is useful to test the algorithm, ideally the swarm will converge on the known best value. When applying the optimizer the n-perturbation approach is used, minimizing the risk for premature convergence.
CSRM multiplies each constant with a random value in [0,1].
Each particle is assigned a small but non-zero velocity. The reason for this is again avoiding premature convergence. Without this velocity all particles are immediately attracted to the first global best. While attraction to this value is desired, it should not dominate the population. The small value of the initial velocity once again reflects an empirical discovered balance between exploration and exploitation.
Each particle is assigned an inertiaweight. % ref inertia 
This value is one approach to combat the velocity explosion problem, which we will cover in \ref{subppsomodification}. 
Finally the global best is recorded. 


\subparagraph{Modification}\label{subppsomodification}
The algorithm updates all particles in sequence, then records the new global best.
Let d be the dimension of the problem, or in our case the number of constants in the tree to optimize.
The velocity v at iteration i of a particle is updated using:
\[
v_{i+1 j} = w_i * v_{ij} + C_1 * (p_{ij} - g_{ij}) * R_1 + C_2 * (p_{ij} - G_{ij}) * R_2 \forall j \in [0, d)
\]
with
\begin{itemize}
\item $v_i$ Current velocity
\item $p_i$ Current position (set of constant values)
\item $g_i$ Local best
\item $G_i$ Global best
\item $C_1$ Constant weight influencing the effect the local best has on the velocity.
\item $C_2$ Constant weight influencing the effect the global best has on the velocity.
\item $w_i$ Inertia weight simulating physical inertia.
\item $R_1$ Random value perturbing the effect the local best has on the velocity.
\item $R_2$ Random value perturbing the effect the global best has on the velocity.
\end{itemize}
Without the inertia weight PSO has issues with velocity explosion, the velocity has a tendency to increase to large values. This increases the distance between particles, but more importantly is far more likely to generate positions that are no longer inside the domain of one or more of the dimensions. Inertia weighting will dampen this effect. % Reference inertia values.
% Inertia.
\\
The position is updated using :
\[
x_{i+1}{j} = x_{ij} + v_{ij} \forall j in [0, d)
\]
The $R_1$ and $R_2$ parameters make the modification stochastic, they introduce perturbations in the calculation. These changes have the benefit that they can break non optimal behavior resulting from the deterministic calculation. If there is a suboptimal best value (local or global) that leads to a too strong attraction and thus forcing premature convergence, we can with a certain probability escape from such a value by perturbing the velocity calculation. The C constants determine how strong the effect is of respectively the local best and the global best. This reflects the balance between exploration and exploitation respectively, where a particle is influenced more by its own information or that of the swarm.
After all particles are updated, the new global best is recorded for the next iteration.

\subparagraph{Selection}
An interesting difference with other algorithms is that the position is always updated, whether it improves the fitness or not. The local best records the best known position, but the particle is allowed to visit positions with lower fitness values. This allows it to escape local optima. The global best is obviously only updated with an improved value. The comparison is strict, meaning that only a better value can become the new global best. This may not seem significant, but allowing equality updates can actually benefit an optimization process. To see this it helps to view the fitness domain as a landscape with troughs, valleys and heights. Allowing for equality updates allows the global best to move despite no apparent improvement in fitness. In this analogy we represent a (sub) optimal value with low points in the landscape.
This allows it to cross areas where a zero gradient is observed, for example between two low points where the lower one is a local optima. 

\paragraph{Cost}
With a population size of n, the algorithm requires n fitness evaluations per iteration. The computational cost of updating the velocity and position is small given the evaluation cost of an expression tree over several datapoints. However, it is linear in d, the number of dimensions. If the tree increases in size, the number of constants can increase in the worst case exponentially. On average due to the construction algorithm we expect that half the leaves in the tree are constants. Given our discussion of the constant folding algorithm we know that a full binary is unlikely, so the number of constant nodes is equally unlikely to increase exponentially, but will nonetheless scale poorly.
In our implementation we will halt the algorithm if it can't improve the global best after k/2 iterations, where k is the maximum number of iterations it can execute. The initialization stage adds another n evaluations, in addition to n per iteration. The total cost in fitness evaluations is therefore n(k+1), resulting in a worst case evaluation complexity of O(nk).

\paragraph{Configuration}
This overview gives the values of each parameter used in CSRM's PSO implementation.
\begin{itemize}
\item $C_1$ = 2 
\item $C_2$ = 2 : Setting both to 2 is recommended as the most generic approach \cite{PSOParameter}.
\item $w_i = \frac{1 + r}{2}$ with r random in [0,1] : In early implementation the inertia weight was kept constant \cite{PSOInertiaShi} There are a large number of strategies for an inertia weight. Dynamically decreasing inertia may improve convergence significantly. 
We opt for a random inertia weight as it has been shown \cite{PSOInertia} to lead to faster convergence. Since our use case requires fast convergence on very limited iterations, this strategy is clearly favored.

\item $R_1, R_2$ r with r random in [0,1]
\item population = 50 : PSO is not sensitive to populations larger than this value, providing a robust default value. \cite{SwarmIntelligence}
\end{itemize}
CSRM's optimizer does not set constraints on the domain of each constant, it is different for each problem instance. 
Finding the domain of a constant in the expression tree requires a domain analysis of the expression tree. With features in the tree involved, finding the exact domain is infeasible, given that some of the datapoints are unknown. It is therefore possible that a particle obtains values outside the valid domain of one or more constants, resulting in an invalid expression tree. This will result in the particle temporarily no longer contributing to the search process.

\subsubsection{DE}
Differential Evolution is a vector based optimization algorithm, or rather as the name implies, it operates by computing the difference between particles. 
% What is it
% Why DE
% How Configure it
\paragraph{Algorithm}
The algorithm has a population of n vectors, similar to the other algorithms it holds a linear set of values to optimize, one per dimension.  
% use DE notation.
\subparagraph{Initialization}
Similar to our approach in initialization PSO, we perturb a known (sub) optimal solution. A vector holds its current values, and the best value. 
\subparagraph{Modification}
Each iteration the algorithm processes all vectors. For each vector x, three distinct randomly selected vectors are selected (with replacement). From these 3 vectors a new 'mutated' vector is obtained:
\[
\vec{v} = \vec{w} + F (\vec{y} - \vec{z})
\]
With w,y,z randomly chosen and not equal to x.\\
From this step the algorithm lends its name. The F factor influences the effect of the difference. 
Then we apply a crossover operation, using vectors x and v and probability parameter Cr.
We select a random index j with $ j \in [0, \vert x \vert)$. We then create a new vector u:
\[
u_i = k_i \forall i \in [0, \vert x \vert
\]
and $k_i$ equal to 
\[   \left\{
\begin{array}{ll}
      v_i & i = j \lor r < Cr \\
      x_i & i \neq j \land r \geq Cr \\
\end{array} 
\right.
\]
\subparagraph{Selection}
For a given selected vector $\vec{x}$ and created vector $\vec{u}$ we now test if $\vec{x}$ is a better candidate than $\vec{u}$, in other words has a lower or equal fitness value. Note the distinction here with PSO, the equality test allows DE vectors to cross areas without a gradient. If f($\vec{u}$) <= $\vec{x}$ the vector is replaced with $\vec{u}$. The global and local best are updated as well. A difference with PSO is that a PSO particle changes regardless of fitness value, whereas in DE the modification is only committed if a better or equal fitness value is obtained. The first approach allows an optimization algorithm to break free from local optima. DE uses the random selection of other vectors to create a similar effect. If we use the landscape analogy, as long as at least one DE vector is outside a depression in the landscape, but all the others are converging to the suboptimal minimum, DE has a probability to escape a local optima. Unlike PSO DE (in our configuration) does not use the global best in its calculations, sharing of information is completely distributed over the vectors.
\paragraph{Cost}
For each vector 3 other vectors are used, or restated we create 2 new vectors. Similar to PSO these calculations have a complexity linear in d, the dimensionality of the problem. The fitness function is called once per iteration per vector. Compared to PSO we therefore have the exact same evaluation complexity of O(nk).

\paragraph{Configuration}
CSRM uses a DE/rand/2/bin configuration. The DE/x/y/z notation reflects its main configuration, where x is the vector perturbed, y is the vectors used in the differential step and z is the crossover operation (binomial). This configuration is referenced \cite{DE} as one of the most competitive for multimodal problems with good convergence to the global optimum. Since we start from a probable local optimum the choice for a random vector instead of the global best vector also helps avoid premature convergence.
This overview gives the values of each parameter used in CSRM's DE implementation.
\begin{itemize}
\item F = 0.6 : F should be in [0.4, 1] Large F values favor exploration, whereas small F values favor exploitation. The value of 0.6 is reported as good starting value. In our problem domain we already have a (sub) optimal solution which we wish to improve, so the risk of premature convergence is high, hence the bias to exploration. 
\item Cr = 0.1 : The Cr values should be in [0,0.2] for separable functions, and [0.9, 1] for non separable functions. We cannot assume dependency between the constants, and therefore use a value of 0.1. This results in DE focussing alongside the axes of each dimension in its search trajectory. 
\end{itemize}
Compared to PSO DE has a low parameter count, and the optimal values for these parameters can be found in literature \cite{DESurveyLatest}. The population size should be t * d with t in [2, 10]. Since we do not know d in advance, and to keep the comparison fair we set the population at 50, allowing for optimal values for up to 25 dimensions (constants).
While DE has a small set of parameters, their effect is still quite pronounced. There exists implementations of DE that use self adapting parameters, but this is beyond our scope. It should be noted that CSRM's optimizer has a very small optimization budget (in evaluation cost) and each new problem has potentially new characteristics. We therefore chose for the most robust values and configuration.