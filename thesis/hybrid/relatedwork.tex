% Related work
\subsection{Symbolic regression compared to other approaches}
Symbolic regression is one technique that fits a model, given a set of input values, to a known output. Other machine learning techniques such as Neural Networks, Support Vector Machines and Random Forests have the same functionality but where SR distinguishes itself is in the white box nature of the model. The convergence characteristics of SR are an active field of study \citep{SRAccur}. New approaches in SR such as GPTIPS and FFX \citep{GPTIPS, FFX} focus on multiple linear regression, where a linear combination of base functions is generated. While GPTIPS still uses GP, FFX is completely deterministic and eschews GP.
In the recent work of \citep{SRlinear} SR is compared with these approaches on a series of benchmark functions, some of which are used in this work. The authors concluded that while SR can have a slower convergence rate compared to conventional machine learning algorithms, the difference is not that large. SR distinguishes itself from other techniques by returning a model that allows for understanding and insight into the process we are approximating.
Symbolic regression has been used to evolve implicit equations \citep{SRimplicit}. The difference with classical SR and other machine learning techniques is striking. As we have seen there are a near infinite number of equivalent equations that fit input to output data. Apart from issues such as representation, selecting a preferred solution is a hard problem, often problem domain dependent. An additional difficulty is providing the algorithm with negatives, values that the surrogate model generated by SR should not produce. These can be used to drastically reduce the number of equivalent solutions and thus increase accuracy. The authors resolved these issues by using a derivative based fitness function in order to find implicit equations that are nontrivial solutions for the datapoints.

\subsection{Algorithms implementing symbolic regression}
Even though SR is usually associated with GP, there exists a wide variety of alternative implementations.
A non GP approach using elements from Grammatical Evolution (GE) \cite{GE}, several genetic algorithm techniques and continuous optimizers (DE, PSO) has been presented in \citep{AEG} with promising results in terms of convergence rate and accuracy. 
It uses a simple C-like expression grammar, including relational and conditional operators. 
The more recently introduced ABC algorithm has also been used for SR \cite{ABCSR}. 
Ant Colony Optimization \citep{ACO} has been used to generate programs \citep{ACOSR}, the same functionality that allows GP to be used for SR.
If an algorithm is capable of evolving programs then it is capable of evolving metaheuristics. This classifies it as an hyperheuristic. This distinction is important because hyperheuristics are not bound by the NFL theorem. In other words there is a free lunch for hyperheuristics \cite{HyperNFL}. 

\subsection{Constant optimization problem}
Several approaches to the constant optimization problem exist. The traditional solution is generating random constants \citep{GP}. With the size of the search space this remains a slow approach.
In \citep{GPConstAlter} a structure based approach is reported to improve the random constant generation process of GP. Each constant is represented not by a single leaf node but by an evolving subtree. Apart from improved convergence, this approach also avoids hybridization of the original GP algorithm. There is a similarity with our approach, where we fold constant subtrees instead of evolving these separately. Their approach underlines our statement that evolving subtrees to generate constants can be quite effective. Our folding approach prevents vanilla GP from doing this, their approach splits constant generation from the GP algorithm but then reuses the same tree evolution techniques to evolve constants.
In \citep{GPConst} the concept of numeric mutation is introduced where constants are mutated separately from the remainder of the tree. This approach is a domain specific variant of applying a continuous optimizer to the constants. It uses a simple implementation based on a temperature-biased normal distribution, inspired by simulated annealing \citep{SA}.
Another approach is made by applying GE to generate constants \citep{GPConstGE}. Several types of grammars are tested that generate and evolve constants.
Hybridization of GP with continuous optimizers is the approach we use in our work. Examples in the field are hybridization with DE \citep{GPDE} and PSO \citep{SRBaseline}.


\subsection{Genetic programming}
A recent study offers a valuable overview of open issues in genetic programming \citep{GPIssues}. This study lists issues we have covered such as problem hardness, fitness topology, problem representation, benchmarks, uncertainty regarding the optimal solution, and constant optimization.
We restrict our work to a simple GP implementation as a baseline for future improvements. Advances such a semantically aware operators \citep {GPSemantics} and modularity are not applied. Modularity started with Koza's \cite{GP} Automatically defined functions (ADF), which allow reuse of partial solutions as base functions and thus signficantly increase the expressiveness of a solution without increasing the representation. This concept is further investigated in \citep{GPModularity} where features such as structure modification and recursion are evaluated. In this work it is shown that with modularity the size of the representation in GP is no longer bound to the number of features. In our work we do not have modularity so we still have a correlation between tree depth and the number of features.
%\citep{GE}
%\citep{GEDE}

\subsection{Parallel symbolic regression}
Enrique Alba's book \cite{parallelmetaheuristics} on parallel metaheuristics is the reference work for the field. It provides a broad overview of the challenges and advantages of parallel metaheuristics.  SR can be implemented using a parallel metaheuristic such as GP.
A Python toolbox for evolutionary algorithms has been developed \citep{DEAP}, not specifically directed at symbolic regression but as a repository of evolutionary algorithms in a distributed context.
An interesting parallel GP SR implementation \citep{DGPSR} introduces a random islands model where processes are allowed to ignore messages, contrary to our approach. The authors argue that this promotes niching, where 'contamination' of locally (per process) fit individuals could otherwise introduce premature convergence. The clear advantage of such a system is speedup, since no process ever waits on other processes. Another difference is the message exchange protocol. Whereas our tool exchanges messages after each phase, their tool uses a probability to decide per process if messages are sent or received interleaved with the generations. Such a setup allows for a heterogeneous set of processes.
Each process would execute a different configuration of an optimization algorithm or even a different algorithm altogether.
This approach tries to mitigate the disadvantages of some algorithms or configuration with the advantages of others. It is an approximation exercise in avoiding the constraint posed by the NFL theorem. A heterogeneous approach can also serve as a self optimizing metaheuristc by testing parameter configurations in parallel or even responding to metadata generated from the convergence process and adjusting these parameters based on findings from other processes. It executes a two layer optimization process: it approximates the original problem and the optimal configuration for solving the problem. % Which is beautiful.
A superlinear speedup is reported for some problems. As we have argued before, ignoring messages introduces non determinism in the algorithm without there being a clear need for this. Niching, islands and prevention of premature convergence can be achieved by other methods. Adding constraints to the fitness function is one approach that promotes niching. Making the distributed algorithm non deterministic makes analysis and experimentation far more complex. 
A different approach is shown in \citep{DFGPSR} where a master slave topology is used in combination with a load balancing algorithm in order to resolve the imbalance between the different slaves executing uneven workloads. The slaves do not form separate processes, they are assigned a subset of the population and execute only the fitness function. The selection and evolution steps are performed by the master process. This a a fine grained approach, and while it offers a speedup in comparison with a sequential GP SR implementation it does not increase the coverage of the search space. The load balancing algorithm is an interesting approach to solve the unavoidable imbalance between the processes. We mitigate this issue with our delay tolerance, but as we have discussed this approach has limits. A master slave topology allows for a centralized approach at the cost of introducing synchronization or even serialization. A distributed variant would be interesting to apply, where the processes distribute the load or alter the communication topology based on their load. Distributed election algorithms could be of use here to elect a coordinator each round. A partitioning of the processes in subgroups based on their location in the topology and communication pattern are other alternatives.
In Distributed Genetic Progamming (DGP) \cite{DGP} a ring and torus topology are used. The two way torus topology is similar to our grid topology. The study finds that sharing of messages is essential to improve convergence but that the communication pattern is largely defined by the problem domain. It concludes that diffusion is a more powerful technique compared to partitioning. In partitioning no communication between subgroups is possible, which can protect against premature convergence.

\subsection{Accuracy and convergence}
The Accuracy and convergence characteristics of SR are an open issue \citep{SRAccuracy, SRAccur, SRBaseline}. A practitioner would like to have certainty regarding the convergence characteristics and accuracy of SR. Given a problem, she would like upper bounds on both when applying SR to the problem. Without these acceptance of SR as a tool in industry will remain difficult.
Finding good benchmark problems for GP is an open issue, recent work \citep{GPBenchmarks} attempts at unifying existing benchmarks and defining standards for existing and new benchmark problems. In this work we use the benchmarks introduced by \cite{SRAccuracy}, hard problems with poor convergence characteristics for a simple GP SR implementation.
A measure estimating problem hardness is introduced in \citep{GPHardness}. This measure is able to predict the effectiveness of operators, or conversely, estimate the hardness of the problem. The measure has a theoretical foundation, it allows a practitioner robust insights into the problem. Problem hardness will affect the convergence characteristics of any metaheuristic, with this hardness measure we can analyze convergence and determine if the algorithm configuration or problem hardness is the cause for poor convergence.
