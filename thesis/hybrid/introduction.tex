\subsection{Overview}
In this work we will study the convergence behavior of Symbolic Regression. The convergence of symbolic regression is still an open problem \cite{SRBaseline, SRAccur, SRAccuracy, FFX}. We will investigate the causes of this and suggest approaches. In particular we tackle the constant optimization problem. We evaluate our approach on a previously introduced \cite{SRBaseline} set of benchmark problems. We parallelize our approach and evaluate the effect on convergence. Finally we test our implementation on a real world use case and study its convergence.
In the remainder of this section will give a problem statement, describe the challenges faced and why they are relevant.
% List sections?
% What we will discuss and where
% Not focussed on what works, but on what doesn't
% No black box, want to see how fitness evolves.

\subsection{Symbolic Regression}
Symbolic regression generates an expression that, when evaluated on a set of input points X, approximates an expected output set. This is a minimization problem where we can use as fitness a distance function. The expression is comprised of base functions, constants and features. 

\subsubsection{Problem hardness}
For any given input set and output set there are an infinite number of expressions that give the exact expected output. If we have a reasonably small set of base functions, a strict limit on the length of the expression, and a finite range for the constants, the number of expressions that matches the output is still unfeasibly large. The set of approximating expressions that come within a threshold distance is obviously even larger. In general we do not have a starting point for the optimization process, forcing us to use a random sample from the search space as initial values.
Let B be the set of base functions with $\vert B \vert = b \land b < \infty$ . The feature set F is user provided and will in practice rarely exceed 1e3: $\vert F \vert = f <= 1e3$. 
If we use $\mathbb{R}$ as the domain for constants we have an infinite search space. Infinite precision floating point numbers have a high performance cost. It is more reasonable to use the IEEE754  double precision floating point standard in order to avoid an infinite search space and to guarantee reasonable performance :  $\vert C \vert = c \sim 2^{64}$.

\paragraph{Problem representation}
If we represent an expression as a tree we can describe the size of the search space. Suppose we allow expressions that are represented by trees with depth d. An internal node is a base function, a leaf is a feature or a constant. If we limit our base function set to binary and unary functions, a full tree of depth d will have on average N nodes:
\[
N = \frac{2^{d+1} - 1 + d}{2} = O(2^d)
\]
The number of leaves is:
\[
L = N - \frac{2^{d} + 1}{2} = O(2^{d-1})
\]
The number of internal nodes for a tree of depth d is given by:
\[
I = N - L = O(2^{d-1})
\]

\paragraph{Size of the search space}\label{searchspace}
For a full random tree of depth d we need to pick I functions with replacement from B.
If we select for a leaf with probability $\frac{1}{2}$ a feature or a constant we need to select L/2 constants from C and L/2 from F, both with replacement.
The total number of trees we can generate with B,C,F and d given is :
\[
S = b^I c^{\frac{L}{2}} f^{\frac{L}{2}}
\]
Clearly c dominates this expression. While the search space is not infinite, it might as well be given our calculation. For any given input and output set we do not know the 'correct' value of d. We do know the maximum number of features the expression can use, and from this could derive a heuristic to determine the expected value d. If we have k features and expect all of them to be significant, we would need at least 2k leaves, with half being features and half constants. This would give us a minimum depth of $\lceil \log_2{2k} \rceil$. In practice the depth should be greater than this value. We cannot rule out that features will need to be reused by an optimal expression. If a tree is not full the number of leaves will be significantly smaller resulting in a higher minimum depth needed to use all features. We have to assume that all features are significant. This reflects once again the incomplete information we have about the search space. Evolutionary algorithms that evolve these expressions are likely to introduce bloat, subexpressions that do not contribute to the fitness value. We have to take this into account if we let f guide our value of d. Suppose we introduce the guide $d = 8 f$ then we can approximate S as :
\[
S = b^{2^{8f}} c^{2^{8f-2}} f^{2^{8f-2}}
\]
With $ c >> b \land c >> f $
\[
S = O(c^{2^f})
\]
The size of the search space and lack of information means we can only approximate this problem with a metaheuristic.

\paragraph{Solution}
We do not know what our solution is. Metaheuristics are typically applied to problem statements where the solution is best described as "I know it when I see it". Only when discovered can we evaluate the worth of a solution. In symbolic regression we could describe our desired solution by requiring that it has a distance 0. This is in and of itself not enough to use in real world applications. We know that the solution will not be unique, so given 2 solutions with an equal distance which do we prefer? The issue of bloat reappears here. Do we prefer simpler expressions above more complex? If h and g both have equal distance but h is continuous and has clearly defined extrema whereas g is discontinuous, is h then a 'better' solution? What happens if we use h and g on unknown data? Do we want only minimum distance or also would like maximum predictive capability? These questions introduce multiobjective fitness functions which use the distance function as only a part of the heuristic driving the algorithm. The diversity between solutions is another aspect that is sometimes a goal in these problem statements. We would like solutions that contain as much unique information as is possible. 


\subsubsection{Compared to other techniques}
When a metaheuristic is applied to a problem and returns a solution a practitioner would like to be able to validate the solution. In general we do not want black box behavior, where the algorithm returns a solution and we do not know how this solution was obtained. A symbolic expression offers a more transparent model to a practitioner than for instance a neural network or a trained classifier. The use of mathematical functions allows insight into the behavior of the model which other approach cannot. From the expression we can deduce how features influence each other and which is more significant. Other insights such as continuity and derivation are more difficult to interpret. We cannot assume that the model we wish to approximate with symbolic regression is continuous simply because our best approximation is continuous.

\subsubsection{Applications}
There are a wide variety of applications for symbolic regression. Simulation is one of the main applications. Simulation of non trivial processes is requires a vast amount of computational resources. To complicate matters, simulators have often a large parameter space.
We can use symbolic regression to gain insight into the correlation between parameters. The resulting expression is a model that approximates the simulator. If we have an accurate model with good predictive capabilities we can save simulations and use the expression instead. The symbolic expression allows us to gain insights into the underlying model. We can use mathematical analysis to distinguish significant parameters and correlated parameters. We will apply SR to a simulation use case to demonstrate the advantages and disadvantages of this application.

\subsection{Convergence}
\subsubsection{Measures}
\paragraph{Quality}
We define the quality of a solution as its fitness value. If we use only the distance function as the fitness value, we can ask the SR tool to return solutions that are at least as fit as a given threshold. This threshold value is hard to set as each problem statement will have different convergence characteristics. In order to use this approach the fitness function should have a finite range with a defined scale. 
\paragraph{Convergence rate}
A practitioner would like to know how long it would take for an SR algorithm to find solutions of a given quality. It is equally important to know how convergence behaves over time. Suppose we have a fitness function with range [0,1] with 0 optimal. The SR tool evolves g generations and the tool and finds a solution with fitness 1e-12. A practitioner would like to know how the increase in generations would reflect on the decrease in fitness. We would like to be able to answer the question : "How many generations are needed to improve fitness by an order of magnitude?". Current implementations in SR are unable to answer these questions. It is possible to track convergence over time and based on this extrapolate future behavior. CSRM offers the user such insights by visualizing the convergence rate amongst other statistics.

\paragraph{Accuracy}
If we know that there exists a single global optimal solution, we can define accuracy as a distance measure. We score each solution the algorithm evolves based on the distance to the known optimal solution. Accuracy measures the algorithm's capability to approximate a known solution. 

\subsection{Metaheuristics}
The symbolic regression problem has an intractable search space with incomplete information. Such a problem statement can be approximated with metaheuristics. In this work we focus on population based algorithms where a set of solutions is updated and the optimization process is accelerated using sharing of information between the solutions.

\subsubsection{Exploration versus exploitation}
All metaheuristics need to find a balance between exploration and exploitation. Exploration is needed in order to have a reasonable probability of finding a neighbourhood in the search space where the global optimum can be found. Exploitation is then needed to find that global optimum, given such a neighbourhood. Exploration has to be limited, by virtue of the problem statement full coverage of the search space is infeasible. Given an intractable or infinite search space exploration should be a sampling process capable of finding regions in the search space that contain optima. Exploration prevents premature convergence, allowing the optimization process to escape from local optima. Finally it promotes diversity. While this is a necessity if we have a multimodal problem statement, diversity is always beneficial for an optimization process. The semantic similarity or difference between solutions with equivalent fitness values offers valuable insights into the underlying problem.
Exploitation will increase the quality of solutions at a risk of overfitting and premature convergence. 

\subsubsection{Analogy with nature}
A large subset of metaheuristics is nature inspired, but care should be taken when using this analogy. Each year new algorithms are introduced based on some observed optimization process in nature, typically a population of social creatures foraging for food. A second analogy is made with evolution itself. Solutions are constructed as genetic material, assigned a fitness function and evolved using several evolution strategies influenced by nature. These approaches have been successfully applied to hard problems, but we have to make an informed application of them.
\paragraph{Fitness}
A first concern is the fitness function. The topology of the fitness function determines to a large extent the convergence of any optimization algorithm. The optimization algorithm's capability to successfully deal with a fitness topology that contains local optima which are hard to escape from is its robustness. A robust optimization algorithm can be applied to a wide set of problems with a low risk of issues such a premature convergence. The problems solved in nature by evolution and cooperation should have a fitness toplogy that at least can be translated to the problem we are trying to solve. A second observation to make regarding fitness is that in nature fitness in and of itself is not the goal of the process, survival is. A genome, animal or insect only has to have sufficient fitness in order to survive. In contrast, in computer science, we are interested in the best, most fit, solution. Both approaches are similar in goal but differ enough to lead to interesting side effects when applying nature inspired algorithms to optimization problems. Genetic algorithms have a tendency to evolve introns, exactly like their counterparts in nature. Introns are large sections of a genome that do not contribute to the fitness. Their role is not completely accidental, however, introns can function as genetic memory and can even help in crossing zero gradient areas in the fitness function topology. % Refs
In optimization algorithms introns cause serious issues. While their beneficial effects can be present here as well, in general introns, or bloat, will only degrade the performance of the algorithm. 
\paragraph{Swarm Intelligence}
A second concern is the lack of novelty when introducing new algorithms based on observations from nature. Most swarm intelligence algorithms are derived from observed behavior in nature where swarms of insects or animals forage for food or defend themselves. 
In computer science such algorithms always feature the same aspects : population management, improving local solutions, discovering new 
solutions and communicating between the particles in the swarm.
Theoretical results for most of these algorithms are lacking or incomplete, yet vital. If we have a theoretical result that guarantees convergence with a resource limit, we have a robust algorithm. This is especially important given that we are trying to solve problems with an intractable search space. The growth in algorithms, and the fact that most of these algorithms have a large set of parameters that impact their convergence makes it difficult for theoretical analysis to keep pace.

\subsubsection{Optimal algorithm}
The No Free Lunch theorem for metaheuristics \cite{NFL} states that no single metaheuristic is optimal for all given problem instances. 
This result can be used as a quality measure for publications in the field of metaheuristics. Some application domains lack benchmark problems that the community can reuse in order to measure new or modified algorithms. 
We can measure the quality of a good set of benchmark problems if no algorithm can be found to be optimal for all problems. This follows from the NFL theorem. A new algorithm that would demonstrate such convergence characteristics would indicate that the problem set is not general enough.

\paragraph{Hyperheuristics}
Hyperheuristics may offer a solution to this issue. A hyperheuristic generates an optimal metaheuristic for a given problem instance, and thus can, in theory, offer optimal solutions for all problem instances \cite{HyperNFL}.
An intermediary approach between optimizing a problem with a metaheuristic, and generating a metaheuristic to optimize a problem, is to optimize an existing metaheuristic. For this we do not need a hyperheuristic, a simple discrete or continuous optimizer suffices. Its input will be the parameter set of the existing metaheuristic we wish to apply to our problem. A variant of this approach is a self optimizing optimizer where the metaheuristic has parameters that are self-adapting based on feedback during the optimization process. As an example, Particle Swarm Optimization (PSO) \cite{PSO} has self adapting variants that have been shown to improve convergence \cite{PSOInertiaShi}. This is an attractive approach as it can be applied during the optimization process and does not require a hyperheuristic. If we are solving a large set of similar optimization problems then it can be more beneficial to generate a metaheuristic using a hyperheuristic. If each problem we face is sufficiently diverse a self adapting metaheuristic is preferred. 

\subsubsection{Combinatorial versus continuous}
The translation or mapping of the problem statement to a representation that can be approximated by a metaheuristic is non trivial.
We differentiate between combinatorial and continuous optimization problems. SR is a combination of a combinatorial and continuous optimization problem. Some metaheuristics are more fit to solve a combinatorial problem, for example Ant Colony Optimization (ACO) applied to the travelling salesman problem (TSP) \cite{ACO}. Others were constructed to focus on continuous problems such as PSO. This does not mean that the distinction is binding, both ACO and PSO have continuous and discrete applications. If our problem combines both a discrete and continuous optimization problems, then using a memetic or hybrid metaheuristic is a useful approach. We combine complementary algorithms in order for them to complement them. A typical example is combining an algorithm known for high exploitation as a local search agent with an algorithm good at exploration for global search \cite{ABCPSO}.
In this work we combine an algorithm for the discrete subproblem with several algorithms that optimize the continuous subproblem. 


\subsubsection{Continuous optimizers}
For the continuous part of the symbolic regression problem we focus on three metaheuristics. All are population based, and can be classified as swarm intelligence. Differential Evolution \cite{DE} works by combining vectors in a multidimensional search space. PSO is, like DE, a well established continuous metaheuristic. PSO focusses on the concept of particles with velocities and position, rather than abstract vectors. Unlike newer algorithms, both have a thorough theoretical analysis that can be used as a guide by a practitioner. Artificial Bee Colony \cite{ABC} is one of the newer algorithms in continuous metaheuristics. The bee analogy is somewhat confusing, it helps to regard ABC simply as a population of particles split of 3 groups where each focusses on exploration or exploitation. All three are robust, performant algorithms with a distinct approach at solving continuous optimization problems. This selection serves as a representative sample for the set of swarm intelligence algorithms. All three have reasonably small parameter sets and published work that studies optimal values for their parameters.

\subsubsection{Genetic Programming}
Genetic programming \cite{GP} evolves not only instances or solutions, but is capable of generating programs. This makes it a hyperheuristic, it can generate a metaheuristic that can optimize a problem, or it can optimize it itself. 
% ref to vanilla GP
% ref to variable length representation
We will cover GP in detail in section \ref{secdesign}  when we discuss our implementation.

\subsection{Constant optimization problem}
From section \ref{searchspace} we know that selecting the 'right' constant value to use in the tree is very hard. The probability of selecting at random a constant even close to an optimal value is extremely small. We can mitigate this by constricting the range of constants. Constant values from a small range can be used by base functions to generate far greater values. We are using subexpressions to help generate the 'right' constant value. This is something an evolutionary algorithm will tend to do by itself. It will try to approximate a constant by combining a few constants with base functions. The problem with this approach is that it is time and space inefficient. The entire subtree can be replaced by a single constant, yet it requires a significant amount of generations to generate the subtree. One approach is to hand over the constant optimization problem to a continuous optimizer and leave the selection of the base functions to the combinatorial optimizer. In section \ref{secconstopt} we will cover this problem in detail.

\subsection{Parallel}
The hardness of the SR problem makes parallelization highly desirable. Regardless of the metaheuristic we apply there are several stages where parallelization can be applied. Evaluating fitness functions will be the main cost of any metaheuristic, obtaining a speedup in this stage would have a great impact on the runtime of the algorithm. This type of parallellization would not alter the behavior of the algorithm, the result would be invariant with or without parallelization. What makes parallel metaheuristics even more interesting is that parallelization can offer improved convergence compared to sequential execution. We can view a set of instances of the algorithm again as a cooperating swarm. The parallel metaheuristic starts to function as a self optimizing metaheuristic. By communicating information about the search space each metaheuristic instance is covering the entire process can accelerate convergence. 
While extremely powerful, parallel metaheuristics themselves require a careful implementation and configuration in order to achieve this goal. From an implementation standpoint we have to consider overhead in messaging and synchronization in order to avoid a serialization effect. The optimization process formed by the parallel metaheuristic has its own parameters that will define its balance between exploration and exploitation. A parallel metaheuristic can also be used as a hyperheuristic, running several instances of a metaheuristic with different parameter values in parallel and evolving the best performing. In section \ref{secdistributed} we will cover this topic in detail.